import os
import re
import time
import json
import torch
import openai
import random
import argparse
import numpy as np
from tqdm import tqdm
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

parser = argparse.ArgumentParser()
parser.add_argument(
    "--input-file",
    type=str,
    default=None,
)
parser.add_argument(
    "--checkpoint",
    type=str,
    default="/path/to/Meta-Llama-3-8B-Instruct",
)
parser.add_argument(
    "--output-file",
    type=str,
    default=None,
)
args = parser.parse_args()

def reflect_line_on_aspects(lines, model, tokenizer):

    def format_prompt_reflection(instruction, response, aspects):

#         prompt_template = """Bellow is an instruction and its corresponding response. Revise the response to adhere with minimal modification to the given constraints while still folowing the instruction. 
# Please start your answer with "**Revised Response**:". Do not generate any other openings, closings or further explanations.
# **Instruction**:
# {instruction}

# **Constraints**:
# {constraints}

# **Response**:
# {response}"""
        prompt_template = """You are provided with a response which is generated by a LLM and some constraints that the response is asked to follow. Now, you have known that the response does not follow the constraints. You are designated as a corrector to correct the response. You should make as minimal revisions as possible so that it follows the constraints. For example, you should not change the case of the word if you are not asked. To fulfil this task, you are expected to provide your analysis and a revised response which has followed the constraints. 
---INPUT---
---response---:
<<Title>>: ISO Code for Andorra
The International Organization for Standardization (ISO) code for Andorra is <<ISO Code: 012>>.
Andorra is a small, independent principality located in the Pyrenees mountains.
I hope this information is helpful! <<Hope you agree with me.>>
---constraints---: 
1. Is the very last sentence of your response "Hope you agree with me."?
2. Does the answer include the definition of ISO Code?
---OUTPUT---
---analysis---:
1. The last sentence of the response is "<<Hope you agree with me.>>", which needs to be changed to "Hope you agree with me." to follow the constraints. 
2. There is no definition of ISO Code included, which needs to be incorporated into the response.
---revised response---:
<<Title>>: ISO Code for Andorra
The International Organization for Standardization (ISO) code for Andorra is <<ISO Code: 012>>.
Andorra is a small, independent principality located in the Pyrenees mountain.
The ISO code is a three-digit number that represents countries.
I hope this information is helpful! Hope you agree with me.
---INPUT---
---response---:
{response}
---constraint---:
{constraints}
---OUTPUT---"""
        constraints = ""
        for i, aspect in enumerate(aspects):
            idx = i + 1
            constraints += f"{idx}. {aspect}\n"
        constraints.strip()
        
        constraints = constraints.strip()
        prompt = prompt_template.format(response=response, constraints=constraints)
        return prompt

    def post_process_reflection(text):
        correct_pattern = r'-?-?-?r?R?evised r?R?esponse-?-?-?:?\n*([\s\S]*)'
        result = re.findall(correct_pattern, text)
        if len(result) == 1:
            return result[0]
        else:
            # print("Parsing failed! Response is "+str(text))
            return ""

    def reflect(lines):

        prompts = [format_prompt_reflection(line['instruction'], line['rejected_response'], line['reflect_aspects']) for line in lines]
        prompts = [tokenizer.apply_chat_template([{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True) for prompt in prompts]

        sampling_params = SamplingParams(max_tokens=4096, temperature=0.5, n=1)
        outputs = model.generate(prompts, sampling_params)
        responses = [output.outputs[0].text for output in outputs]
        responses = [post_process_reflection(resp) for resp in responses]

        return responses

    for line in lines:

        i = np.argsort(line["scores"])[0]
        line['rejected_response'] = line['responses'][i]
        aspect_score = line['aspect_scores'][i]
        line['reflect_aspects'] = []
        for j, score in enumerate(aspect_score):
            if score == 0:
                line['reflect_aspects'].append(line['aspects'][j])
    
    responses = reflect(lines)

    new_lines = []
    for i, response in enumerate(responses):
        chosen_response = response
        rejected_response = lines[i]['rejected_response']
        if chosen_response != rejected_response and chosen_response != "":
            new_line = {
                "instruction": lines[i]["instruction"],
                "input": "",
                "chosen": chosen_response,
                "rejected": rejected_response,
                "history": ""
            }
            new_lines.append(new_line)
    
    return new_lines

if __name__ == "__main__":

    fin = open(args.input_file, "r")
    lines = [json.loads(line.strip()) for line in fin.readlines()]
    lines = [line for line in lines if sum(line['scores']) != 4]

    insts = set([line['instruction'] for line in lines])

    model = LLM(model=args.checkpoint, tensor_parallel_size=1, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint, trust_remote_code=True)

    lines = reflect_line_on_aspects(lines, model, tokenizer)

    sample_lines = random.sample(lines, k=3)
    for i in range(len(sample_lines)):
        sample_line = sample_lines[i]
        print(f"*************Sampled Line {i}*************")
        print(json.dumps(sample_line, indent=4))
    
    print("Totally "+str(len(lines))+" lines!")

    with open(args.output_file, "w") as fout:
        json.dump(lines, fout, indent=4, ensure_ascii=False)
